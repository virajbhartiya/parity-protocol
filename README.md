# Parity Protocol

[![Go Version](https://img.shields.io/badge/Go-1.22.7%2B-00ADD8?style=flat-square&logo=go)](https://go.dev/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-14.0%2B-336791?style=flat-square&logo=postgresql)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Required-2496ED?style=flat-square&logo=docker)](https://www.docker.com/)
[![Filecoin](https://img.shields.io/badge/Filecoin-Calibration-0090FF?style=flat-square&logo=filecoin)](https://filecoin.io/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg?style=flat-square)](LICENSE)

## üöÄ Introduction

**Parity Protocol** is a cutting-edge distributed computation network that enables secure and verifiable task execution across verified nodes. Originally built as a foundational distributed compute platform, **we have significantly enhanced it during the hackathon** with advanced federated learning capabilities, LLM inference services, reputation-based monitoring, and IPFS/Filecoin distributed storage.

## üéØ What Was Already There (Pre-Hackathon)

The **foundational Parity Protocol infrastructure** was already established and included:

- ‚úÖ **Distributed Task Execution**: Secure Docker-based task processing
- ‚úÖ **Node Management**: Runner registration and task distribution
- ‚úÖ **Blockchain Integration**: ERC-20 token economics and staking
- ‚úÖ **Wallet Infrastructure**: Secure token management and transfers
- ‚úÖ **Core APIs**: RESTful interfaces for task submission and monitoring

## üÜï What We Added During Hackathon

We transformed Parity Protocol into a comprehensive **AI and ML platform** by adding:

### ü§ñ **Federated Learning System**

- **Privacy-Preserving ML**: Train models without centralizing data
- **Multiple Aggregation Methods**: FedAvg, FedProx, and custom algorithms
- **Data Partitioning**: Non-IID, label skew, and stratified distribution
- **Differential Privacy**: Advanced privacy protection mechanisms
- **Real-time Coordination**: Distributed training across multiple runners

üìñ [**Federated Learning Guide**](documentation/FEDERATED_LEARNING_GUIDE.md)

### üß† **LLM Inference Network**

- **Distributed AI**: Ollama-powered LLM hosting across runners
- **Model Discovery**: Automatic detection of available models
- **Load Balancing**: Intelligent request distribution
- **Usage Analytics**: Comprehensive billing and metrics
- **Multi-Model Support**: Llama, Mistral, CodeLlama, and more

üìñ [**LLM Models Guide**](documentation/LLM_MODELS_GUIDE.md)

### üõ°Ô∏è **Reputation & Monitoring System**

- **Peer-to-Peer Monitoring**: Runners monitor each other for quality
- **Automatic Slashing**: Bad actors lose staked tokens
- **Quality Metrics**: Performance and reliability scoring
- **Malicious Behavior Detection**: Advanced anomaly detection

üìñ [**FL Infrastructure Quality**](documentation/FL_INFRASTRUCTURE_QUALITY.md)

### üìÅ **IPFS/Filecoin Storage**

- **Distributed Data**: Decentralized storage for training datasets
- **Content Addressing**: Immutable data references via CIDs
- **Upload/Download**: Seamless integration with FL workflows
- **Storage Economics**: Filecoin-based data persistence

üìñ [**Data Partitioning Guide**](documentation/DATA_PARTITIONING_GUIDE.md)

### üí∞ **Enhanced Economics**

- **USDFC Rewards**: Real blockchain payments for ML contributions
- **FL-Specific Incentives**: Rewards based on data quality and participation
- **Performance Bonuses**: Higher rewards for better contributions
- **Session Completion Bonuses**: Long-term participation incentives

üìñ [**FL Rewards System**](documentation/FL_REWARDS_SYSTEM.md)

## üèóÔ∏è System Architecture

### Core Components (Pre-Existing)

| Component            | Repository                                                                | Purpose                           |
| -------------------- | ------------------------------------------------------------------------- | --------------------------------- |
| üñ•Ô∏è **parity-server** | [theblitlabs/parity-server](https://github.com/theblitlabs/parity-server) | Task orchestration and validation |
| üèÉ **parity-runner** | [theblitlabs/parity-runner](https://github.com/theblitlabs/parity-runner) | Secure task execution in Docker   |
| üîå **parity-client** | [theblitlabs/parity-client](https://github.com/theblitlabs/parity-client) | Task submission and monitoring    |
| üíé **parity-token**  | [theblitlabs/parity-token](https://github.com/theblitlabs/parity-token)   | ERC20 token and economics         |
| üëõ **parity-wallet** | [theblitlabs/parity-wallet](https://github.com/theblitlabs/parity-wallet) | Secure token management           |

### New Features Added

```mermaid
graph TB
    Client[parity-client] --> Server[parity-server]
    Server --> Runner1[parity-runner 1]
    Server --> Runner2[parity-runner 2]
    Server --> RunnerN[parity-runner N]

    Server --> FL[Federated Learning]
    Server --> LLM[LLM Inference]
    Server --> Rep[Reputation System]

    FL --> IPFS[IPFS Storage]
    Runner1 --> Ollama1[Ollama Models]
    Runner2 --> Ollama2[Ollama Models]

    Rep --> Monitor[Peer Monitoring]
    Monitor --> Slash[Auto Slashing]

    subgraph "New Hackathon Features"
        FL
        LLM
        Rep
        IPFS
        Ollama1
        Ollama2
        Monitor
        Slash
    end
```

## üöÄ Quick Start

### Prerequisites

- **Go 1.22.7+**
- **PostgreSQL 14.0+**
- **Docker** with Docker Compose
- **Ollama** (for LLM inference)
- **USDFC tokens** on Filecoin Calibration Network

### 1. Clone and Setup

```bash
# Clone the main repository
git clone https://github.com/your-org/parity-protocol.git
cd parity-protocol

# Setup individual components (see parity-protocol/README.md for details)
```

### 2. Start Core Services

```bash
# Start server (with new FL and LLM capabilities)
cd parity-server
./parity-server server

# Start runner (with FL training and LLM hosting)
cd parity-runner
./parity-runner runner

# Start Ollama for LLM inference
ollama serve
```

### 3. Try New Features

```bash
# Submit LLM inference request
./parity-client llm submit --prompt "Explain quantum computing" --model "llama2"

# Create federated learning session
./parity-client fl create-session-with-data ./dataset.csv \
  --name "Image Classification" \
  --model-type neural_network \
  --total-rounds 5

# Check reputation status
./parity-client reputation status
```

## üìö Documentation

### User Guides

- ü§ñ [**Federated Learning Guide**](documentation/FEDERATED_LEARNING_GUIDE.md) - Complete FL user manual
- üß† [**LLM Models Guide**](documentation/LLM_MODELS_GUIDE.md) - LLM inference and hosting
- üìä [**Data Partitioning Guide**](documentation/DATA_PARTITIONING_GUIDE.md) - Data distribution strategies

### Operations & Infrastructure

- üöÄ [**Deployment Guide**](documentation/DEPLOYMENT_GUIDE.md) - Complete system deployment
- üí∞ [**FL Rewards System**](documentation/FL_REWARDS_SYSTEM.md) - Economic incentives
- üèóÔ∏è [**FL Infrastructure Quality**](documentation/FL_INFRASTRUCTURE_QUALITY.md) - Quality framework

### Component Documentation

- üìñ [**Parity Protocol Core**](parity-protocol/README.md) - Original system documentation
- üîó Individual component READMEs in respective repositories

## üéÆ Use Cases

### Federated Learning

- **Healthcare**: Train medical models without sharing patient data
- **Finance**: Collaborative fraud detection across institutions
- **IoT**: Edge device learning with privacy preservation
- **Research**: Multi-institutional AI research collaboration

### LLM Inference

- **Distributed AI**: Decentralized language model hosting
- **Cost Optimization**: Efficient resource utilization across network
- **Model Diversity**: Access to multiple specialized models
- **Censorship Resistance**: Distributed inference prevents single points of failure

### Reputation & Quality

- **Network Security**: Automated detection and punishment of bad actors
- **Quality Assurance**: Continuous monitoring of participant performance
- **Trust Building**: Transparent reputation system for all participants

## ü§ù Contributing

We welcome contributions to both the **original Parity Protocol infrastructure** and the **new hackathon features**!

### Areas for Contribution

**Core Platform** (Original):

- Task execution optimizations
- Wallet and token improvements
- Infrastructure scaling

**New Features** (Hackathon Additions):

- New FL aggregation algorithms
- Additional LLM model support
- Enhanced reputation mechanisms
- Storage optimizations

### Get Started

1. **Choose a Component**: Pick from server, runner, client, or documentation
2. **Check Issues**: Look for open issues in the relevant repository
3. **Follow Guidelines**: Use [Conventional Commits](https://www.conventionalcommits.org/)
4. **Test Thoroughly**: Ensure both original and new features work

## üìä Network Stats

- **Blockchain**: Filecoin Calibration Network
- **Token**: USDFC (ERC-20)
- **Storage**: IPFS/Filecoin distributed storage
- **Privacy**: Differential privacy for FL
- **Models**: 20+ supported LLM models via Ollama

## üèÜ Hackathon Achievements

During the hackathon, we successfully transformed Parity Protocol from a general distributed compute platform into a **comprehensive AI infrastructure** with:

- **Full Federated Learning Pipeline** - From data upload to model training
- **Production LLM Network** - Real distributed inference capabilities
- **Automated Quality Control** - Peer monitoring and reputation management
- **Decentralized Storage** - IPFS/Filecoin integration for data persistence
- **Real Economic Incentives** - USDFC token rewards for AI contributions

**The result**: A production-ready platform for privacy-preserving AI that combines the best of decentralized compute, federated learning, and token economics.

---

üöÄ **Ready to build the future of distributed AI?** Start with our [Deployment Guide](documentation/DEPLOYMENT_GUIDE.md)!
